{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ZerXXX0/sales-prediction/blob/main/FTTransformer_MLQ.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "id": "_aNvUklR5gyB"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.impute import SimpleImputer\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "id": "H8msW7r24-lU"
      },
      "outputs": [],
      "source": [
        "train_df = pd.read_csv('https://raw.githubusercontent.com/ZerXXX0/sales-prediction/refs/heads/main/dataset/train_final.csv')\n",
        "test_df = pd.read_csv('https://raw.githubusercontent.com/ZerXXX0/sales-prediction/refs/heads/main/dataset/test_final.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "id": "lXH0Q-j_5jH_"
      },
      "outputs": [],
      "source": [
        "# === Preprocessing ===\n",
        "drop_cols = ['Unnamed: 0', 'TransactionID', 'MemberID']\n",
        "train_df_clean = train_df.drop(columns=drop_cols)\n",
        "\n",
        "X = train_df_clean.drop(columns=['next_buy'])\n",
        "y = train_df_clean['next_buy']\n",
        "\n",
        "# ✅ New: exclude target column\n",
        "train_cols = [col for col in train_df_clean.columns if col not in ['next_buy']]\n",
        "\n",
        "imputer = SimpleImputer(strategy='mean')\n",
        "X_imputed = imputer.fit_transform(X)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X_imputed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {
        "id": "M61o7MC75vYj"
      },
      "outputs": [],
      "source": [
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X_scaled, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
        "y_train_tensor = torch.tensor(y_train.values, dtype=torch.long)\n",
        "X_val_tensor = torch.tensor(X_val, dtype=torch.float32)\n",
        "y_val_tensor = torch.tensor(y_val.values, dtype=torch.long)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {
        "id": "GGoe1CQU6CnL"
      },
      "outputs": [],
      "source": [
        "train_loader = DataLoader(TensorDataset(X_train_tensor, y_train_tensor), batch_size=1024, shuffle=True)\n",
        "val_loader = DataLoader(TensorDataset(X_val_tensor, y_val_tensor), batch_size=1024)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {
        "id": "L6YDOUyX6G3K"
      },
      "outputs": [],
      "source": [
        "# === FT-Transformer Model ===\n",
        "class FTTransformer(nn.Module):\n",
        "    def __init__(self, input_dim, dim=64, depth=3, heads=4, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Linear(input_dim, dim)\n",
        "        encoder_layer = nn.TransformerEncoderLayer(d_model=dim, nhead=heads, dropout=dropout, batch_first=True)\n",
        "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=depth)\n",
        "        self.cls_token = nn.Parameter(torch.randn(1, 1, dim))\n",
        "        self.head = nn.Sequential(\n",
        "            nn.LayerNorm(dim),\n",
        "            nn.Linear(dim, 2)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        B = x.shape[0]\n",
        "        x = self.embedding(x).unsqueeze(1)\n",
        "        cls_tokens = self.cls_token.expand(B, -1, -1)\n",
        "        x = torch.cat((cls_tokens, x), dim=1)\n",
        "        x = self.transformer(x)\n",
        "        return self.head(x[:, 0])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {},
      "outputs": [],
      "source": [
        "# class balancing\n",
        "classes = np.unique(y_train)  # ensures correct classes\n",
        "class_weights = compute_class_weight(class_weight='balanced', classes=classes, y=y_train)\n",
        "class_weights_tensor = torch.tensor(class_weights, dtype=torch.float32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {
        "id": "IsIdzGC46LCZ"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "FTTransformer(\n",
              "  (embedding): Linear(in_features=8, out_features=128, bias=True)\n",
              "  (transformer): TransformerEncoder(\n",
              "    (layers): ModuleList(\n",
              "      (0-3): 4 x TransformerEncoderLayer(\n",
              "        (self_attn): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
              "        )\n",
              "        (linear1): Linear(in_features=128, out_features=2048, bias=True)\n",
              "        (dropout): Dropout(p=0.2, inplace=False)\n",
              "        (linear2): Linear(in_features=2048, out_features=128, bias=True)\n",
              "        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "        (dropout1): Dropout(p=0.2, inplace=False)\n",
              "        (dropout2): Dropout(p=0.2, inplace=False)\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (head): Sequential(\n",
              "    (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "    (1): Linear(in_features=128, out_features=2, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 90,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# === Training Loop ===\n",
        "model = FTTransformer(input_dim=X_train.shape[1],\n",
        "                      dim=128,         # increase hidden size\n",
        "                      depth=4,         # more transformer layers\n",
        "                      heads=8,         # more attention heads\n",
        "                      dropout=0.2      # add more regularization\n",
        "                      )\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "criterion = nn.CrossEntropyLoss(weight=class_weights_tensor.to(device))\n",
        "optimizer = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', patience=3, factor=0.5)\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {
        "id": "JTBAv5v76N24"
      },
      "outputs": [],
      "source": [
        "def train_model(model, train_loader, val_loader, epochs=50, save_path=\"best_ft_transformer.pth\"):\n",
        "    best_loss = float('inf')\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "\n",
        "        for xb, yb in train_loader:\n",
        "            xb, yb = xb.to(device), yb.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            preds = model(xb)\n",
        "            loss = criterion(preds, yb)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        # === Validation ===\n",
        "        model.eval()\n",
        "        val_loss = 0\n",
        "        correct = total = 0\n",
        "        with torch.no_grad():\n",
        "            for xb, yb in val_loader:\n",
        "                xb, yb = xb.to(device), yb.to(device)\n",
        "                preds = model(xb)\n",
        "                loss = criterion(preds, yb)\n",
        "                val_loss += loss.item()\n",
        "\n",
        "                predicted = torch.argmax(preds, dim=1)\n",
        "                correct += (predicted == yb).sum().item()\n",
        "                total += yb.size(0)\n",
        "\n",
        "        val_loss /= len(val_loader)\n",
        "        val_acc = correct / total * 100\n",
        "\n",
        "        # Scheduler step on validation accuracy\n",
        "        scheduler.step(val_acc)\n",
        "\n",
        "        # Save the model with the lowest val loss\n",
        "        if val_loss < best_loss:\n",
        "            best_loss = val_loss\n",
        "            torch.save(model.state_dict(), save_path)\n",
        "            print(f\"✅ Saved new best model (epoch {epoch+1}, val_loss: {val_loss:.4f}, val_acc: {val_acc:.2f}%)\")\n",
        "\n",
        "        print(f\"Epoch {epoch+1}: Train Loss={total_loss:.4f}, Val Loss={val_loss:.4f}, Val Acc={val_acc:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MHyGegJY6e00",
        "outputId": "1155e16e-1106-444a-ff57-965e0f19da49"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Saved new best model (epoch 1, val_loss: 0.4797, val_acc: 65.63%)\n",
            "Epoch 1: Train Loss=58.7218, Val Loss=0.4797, Val Acc=65.63%\n",
            "✅ Saved new best model (epoch 2, val_loss: 0.4782, val_acc: 68.44%)\n",
            "Epoch 2: Train Loss=49.2192, Val Loss=0.4782, Val Acc=68.44%\n",
            "✅ Saved new best model (epoch 3, val_loss: 0.4664, val_acc: 69.81%)\n",
            "Epoch 3: Train Loss=48.9498, Val Loss=0.4664, Val Acc=69.81%\n",
            "Epoch 4: Train Loss=48.2853, Val Loss=0.4668, Val Acc=67.73%\n",
            "✅ Saved new best model (epoch 5, val_loss: 0.4606, val_acc: 70.31%)\n",
            "Epoch 5: Train Loss=48.0459, Val Loss=0.4606, Val Acc=70.31%\n",
            "Epoch 6: Train Loss=47.9798, Val Loss=0.4623, Val Acc=74.11%\n",
            "Epoch 7: Train Loss=47.8494, Val Loss=0.4626, Val Acc=71.74%\n",
            "✅ Saved new best model (epoch 8, val_loss: 0.4587, val_acc: 71.27%)\n",
            "Epoch 8: Train Loss=47.5241, Val Loss=0.4587, Val Acc=71.27%\n",
            "Epoch 9: Train Loss=47.8203, Val Loss=0.4661, Val Acc=73.37%\n",
            "Epoch 10: Train Loss=47.4492, Val Loss=0.4617, Val Acc=69.95%\n",
            "✅ Saved new best model (epoch 11, val_loss: 0.4513, val_acc: 73.83%)\n",
            "Epoch 11: Train Loss=47.0684, Val Loss=0.4513, Val Acc=73.83%\n",
            "Epoch 12: Train Loss=47.0262, Val Loss=0.4530, Val Acc=73.28%\n",
            "Epoch 13: Train Loss=46.9581, Val Loss=0.4553, Val Acc=73.93%\n",
            "Epoch 14: Train Loss=46.6867, Val Loss=0.4542, Val Acc=70.23%\n",
            "✅ Saved new best model (epoch 15, val_loss: 0.4477, val_acc: 71.05%)\n",
            "Epoch 15: Train Loss=46.4271, Val Loss=0.4477, Val Acc=71.05%\n",
            "✅ Saved new best model (epoch 16, val_loss: 0.4471, val_acc: 72.04%)\n",
            "Epoch 16: Train Loss=46.2349, Val Loss=0.4471, Val Acc=72.04%\n",
            "Epoch 17: Train Loss=46.2075, Val Loss=0.4476, Val Acc=71.97%\n",
            "Epoch 18: Train Loss=46.2695, Val Loss=0.4512, Val Acc=72.79%\n",
            "✅ Saved new best model (epoch 19, val_loss: 0.4469, val_acc: 72.24%)\n",
            "Epoch 19: Train Loss=46.0362, Val Loss=0.4469, Val Acc=72.24%\n",
            "✅ Saved new best model (epoch 20, val_loss: 0.4455, val_acc: 71.98%)\n",
            "Epoch 20: Train Loss=45.9606, Val Loss=0.4455, Val Acc=71.98%\n",
            "Epoch 21: Train Loss=45.9249, Val Loss=0.4458, Val Acc=72.76%\n",
            "✅ Saved new best model (epoch 22, val_loss: 0.4443, val_acc: 72.70%)\n",
            "Epoch 22: Train Loss=45.7708, Val Loss=0.4443, Val Acc=72.70%\n",
            "✅ Saved new best model (epoch 23, val_loss: 0.4438, val_acc: 72.07%)\n",
            "Epoch 23: Train Loss=45.6568, Val Loss=0.4438, Val Acc=72.07%\n",
            "Epoch 24: Train Loss=45.6161, Val Loss=0.4452, Val Acc=72.55%\n",
            "Epoch 25: Train Loss=45.4853, Val Loss=0.4444, Val Acc=72.53%\n",
            "Epoch 26: Train Loss=45.6074, Val Loss=0.4438, Val Acc=71.90%\n",
            "Epoch 27: Train Loss=45.5435, Val Loss=0.4440, Val Acc=72.62%\n",
            "✅ Saved new best model (epoch 28, val_loss: 0.4429, val_acc: 73.05%)\n",
            "Epoch 28: Train Loss=45.4100, Val Loss=0.4429, Val Acc=73.05%\n",
            "✅ Saved new best model (epoch 29, val_loss: 0.4418, val_acc: 72.62%)\n",
            "Epoch 29: Train Loss=45.5268, Val Loss=0.4418, Val Acc=72.62%\n",
            "Epoch 30: Train Loss=45.4579, Val Loss=0.4431, Val Acc=72.43%\n",
            "Epoch 31: Train Loss=45.4300, Val Loss=0.4419, Val Acc=72.53%\n",
            "Epoch 32: Train Loss=45.3698, Val Loss=0.4419, Val Acc=72.55%\n",
            "Epoch 33: Train Loss=45.3909, Val Loss=0.4421, Val Acc=72.65%\n",
            "Epoch 34: Train Loss=45.4301, Val Loss=0.4424, Val Acc=72.63%\n",
            "Epoch 35: Train Loss=45.4500, Val Loss=0.4421, Val Acc=72.64%\n",
            "Epoch 36: Train Loss=45.4191, Val Loss=0.4419, Val Acc=72.78%\n",
            "Epoch 37: Train Loss=45.3073, Val Loss=0.4423, Val Acc=72.53%\n",
            "Epoch 38: Train Loss=45.3312, Val Loss=0.4420, Val Acc=72.57%\n",
            "Epoch 39: Train Loss=45.3113, Val Loss=0.4421, Val Acc=72.73%\n",
            "Epoch 40: Train Loss=45.3162, Val Loss=0.4421, Val Acc=72.73%\n",
            "Epoch 41: Train Loss=45.2826, Val Loss=0.4420, Val Acc=72.71%\n",
            "✅ Saved new best model (epoch 42, val_loss: 0.4418, val_acc: 72.75%)\n",
            "Epoch 42: Train Loss=45.2752, Val Loss=0.4418, Val Acc=72.75%\n",
            "Epoch 43: Train Loss=45.3043, Val Loss=0.4420, Val Acc=72.69%\n",
            "Epoch 44: Train Loss=45.3065, Val Loss=0.4420, Val Acc=72.70%\n",
            "Epoch 45: Train Loss=45.2881, Val Loss=0.4420, Val Acc=72.70%\n",
            "Epoch 46: Train Loss=45.3319, Val Loss=0.4421, Val Acc=72.59%\n",
            "Epoch 47: Train Loss=45.2913, Val Loss=0.4420, Val Acc=72.62%\n",
            "Epoch 48: Train Loss=45.4596, Val Loss=0.4420, Val Acc=72.66%\n",
            "Epoch 49: Train Loss=45.3414, Val Loss=0.4420, Val Acc=72.67%\n",
            "Epoch 50: Train Loss=45.3168, Val Loss=0.4420, Val Acc=72.65%\n",
            "Epoch 51: Train Loss=45.3886, Val Loss=0.4420, Val Acc=72.65%\n",
            "Epoch 52: Train Loss=45.3955, Val Loss=0.4420, Val Acc=72.65%\n",
            "Epoch 53: Train Loss=45.4097, Val Loss=0.4420, Val Acc=72.65%\n",
            "Epoch 54: Train Loss=45.3940, Val Loss=0.4419, Val Acc=72.66%\n",
            "Epoch 55: Train Loss=45.3627, Val Loss=0.4419, Val Acc=72.67%\n",
            "Epoch 56: Train Loss=45.3284, Val Loss=0.4419, Val Acc=72.67%\n",
            "Epoch 57: Train Loss=45.2859, Val Loss=0.4419, Val Acc=72.65%\n",
            "Epoch 58: Train Loss=45.3779, Val Loss=0.4420, Val Acc=72.66%\n",
            "Epoch 59: Train Loss=45.3117, Val Loss=0.4420, Val Acc=72.66%\n",
            "Epoch 60: Train Loss=45.3739, Val Loss=0.4420, Val Acc=72.66%\n",
            "Epoch 61: Train Loss=45.2589, Val Loss=0.4420, Val Acc=72.66%\n",
            "Epoch 62: Train Loss=45.3402, Val Loss=0.4420, Val Acc=72.65%\n",
            "Epoch 63: Train Loss=45.3639, Val Loss=0.4420, Val Acc=72.65%\n",
            "Epoch 64: Train Loss=45.2826, Val Loss=0.4420, Val Acc=72.64%\n",
            "Epoch 65: Train Loss=45.2545, Val Loss=0.4420, Val Acc=72.64%\n",
            "Epoch 66: Train Loss=45.3880, Val Loss=0.4420, Val Acc=72.65%\n",
            "Epoch 67: Train Loss=45.2549, Val Loss=0.4420, Val Acc=72.65%\n",
            "Epoch 68: Train Loss=45.2987, Val Loss=0.4420, Val Acc=72.65%\n",
            "Epoch 69: Train Loss=45.3003, Val Loss=0.4420, Val Acc=72.65%\n",
            "Epoch 70: Train Loss=45.3116, Val Loss=0.4420, Val Acc=72.65%\n",
            "Epoch 71: Train Loss=45.2482, Val Loss=0.4419, Val Acc=72.65%\n",
            "Epoch 72: Train Loss=45.4230, Val Loss=0.4419, Val Acc=72.65%\n",
            "Epoch 73: Train Loss=45.4185, Val Loss=0.4419, Val Acc=72.65%\n",
            "Epoch 74: Train Loss=45.3879, Val Loss=0.4419, Val Acc=72.65%\n",
            "Epoch 75: Train Loss=45.2924, Val Loss=0.4420, Val Acc=72.65%\n",
            "Epoch 76: Train Loss=45.3691, Val Loss=0.4419, Val Acc=72.65%\n",
            "Epoch 77: Train Loss=45.3469, Val Loss=0.4419, Val Acc=72.65%\n",
            "Epoch 78: Train Loss=45.4460, Val Loss=0.4419, Val Acc=72.65%\n",
            "Epoch 79: Train Loss=45.3743, Val Loss=0.4419, Val Acc=72.65%\n",
            "Epoch 80: Train Loss=45.3535, Val Loss=0.4419, Val Acc=72.65%\n",
            "Epoch 81: Train Loss=45.3578, Val Loss=0.4419, Val Acc=72.65%\n",
            "Epoch 82: Train Loss=45.3422, Val Loss=0.4419, Val Acc=72.65%\n",
            "Epoch 83: Train Loss=45.2220, Val Loss=0.4419, Val Acc=72.65%\n",
            "Epoch 84: Train Loss=45.4044, Val Loss=0.4419, Val Acc=72.65%\n",
            "Epoch 85: Train Loss=45.3448, Val Loss=0.4419, Val Acc=72.65%\n",
            "Epoch 86: Train Loss=45.2948, Val Loss=0.4419, Val Acc=72.65%\n",
            "Epoch 87: Train Loss=45.3190, Val Loss=0.4419, Val Acc=72.65%\n",
            "Epoch 88: Train Loss=45.2788, Val Loss=0.4419, Val Acc=72.65%\n",
            "Epoch 89: Train Loss=45.3547, Val Loss=0.4419, Val Acc=72.65%\n",
            "Epoch 90: Train Loss=45.3551, Val Loss=0.4419, Val Acc=72.65%\n",
            "Epoch 91: Train Loss=45.2550, Val Loss=0.4419, Val Acc=72.65%\n",
            "Epoch 92: Train Loss=45.3791, Val Loss=0.4419, Val Acc=72.65%\n",
            "Epoch 93: Train Loss=45.4800, Val Loss=0.4419, Val Acc=72.65%\n",
            "Epoch 94: Train Loss=45.2470, Val Loss=0.4419, Val Acc=72.65%\n",
            "Epoch 95: Train Loss=45.3135, Val Loss=0.4419, Val Acc=72.65%\n",
            "Epoch 96: Train Loss=45.2973, Val Loss=0.4419, Val Acc=72.65%\n",
            "Epoch 97: Train Loss=45.3729, Val Loss=0.4419, Val Acc=72.64%\n",
            "Epoch 98: Train Loss=45.3185, Val Loss=0.4419, Val Acc=72.64%\n",
            "Epoch 99: Train Loss=45.3357, Val Loss=0.4419, Val Acc=72.64%\n",
            "Epoch 100: Train Loss=45.2422, Val Loss=0.4419, Val Acc=72.64%\n",
            "Epoch 101: Train Loss=45.2759, Val Loss=0.4419, Val Acc=72.64%\n",
            "Epoch 102: Train Loss=45.3505, Val Loss=0.4419, Val Acc=72.64%\n",
            "Epoch 103: Train Loss=45.2516, Val Loss=0.4419, Val Acc=72.64%\n",
            "Epoch 104: Train Loss=45.2752, Val Loss=0.4419, Val Acc=72.64%\n",
            "Epoch 105: Train Loss=45.2639, Val Loss=0.4419, Val Acc=72.65%\n",
            "Epoch 106: Train Loss=45.2347, Val Loss=0.4419, Val Acc=72.65%\n",
            "Epoch 107: Train Loss=45.2789, Val Loss=0.4419, Val Acc=72.65%\n",
            "Epoch 108: Train Loss=45.3155, Val Loss=0.4419, Val Acc=72.65%\n",
            "Epoch 109: Train Loss=45.2955, Val Loss=0.4419, Val Acc=72.65%\n",
            "Epoch 110: Train Loss=45.3106, Val Loss=0.4419, Val Acc=72.64%\n",
            "Epoch 111: Train Loss=45.3831, Val Loss=0.4419, Val Acc=72.65%\n",
            "Epoch 112: Train Loss=45.2492, Val Loss=0.4419, Val Acc=72.64%\n",
            "Epoch 113: Train Loss=45.4628, Val Loss=0.4419, Val Acc=72.65%\n",
            "Epoch 114: Train Loss=45.3824, Val Loss=0.4419, Val Acc=72.65%\n",
            "Epoch 115: Train Loss=45.3326, Val Loss=0.4419, Val Acc=72.64%\n",
            "Epoch 116: Train Loss=45.2849, Val Loss=0.4419, Val Acc=72.65%\n",
            "Epoch 117: Train Loss=45.2032, Val Loss=0.4419, Val Acc=72.64%\n",
            "Epoch 118: Train Loss=45.2939, Val Loss=0.4419, Val Acc=72.64%\n",
            "Epoch 119: Train Loss=45.2879, Val Loss=0.4419, Val Acc=72.65%\n",
            "Epoch 120: Train Loss=45.3051, Val Loss=0.4419, Val Acc=72.65%\n",
            "Epoch 121: Train Loss=45.4819, Val Loss=0.4419, Val Acc=72.65%\n",
            "Epoch 122: Train Loss=45.4110, Val Loss=0.4419, Val Acc=72.65%\n",
            "Epoch 123: Train Loss=45.2818, Val Loss=0.4419, Val Acc=72.65%\n",
            "Epoch 124: Train Loss=45.3455, Val Loss=0.4419, Val Acc=72.65%\n",
            "Epoch 125: Train Loss=45.3202, Val Loss=0.4419, Val Acc=72.65%\n",
            "Epoch 126: Train Loss=45.3483, Val Loss=0.4419, Val Acc=72.65%\n",
            "Epoch 127: Train Loss=45.3518, Val Loss=0.4419, Val Acc=72.65%\n",
            "Epoch 128: Train Loss=45.2561, Val Loss=0.4419, Val Acc=72.65%\n",
            "Epoch 129: Train Loss=45.3765, Val Loss=0.4419, Val Acc=72.65%\n",
            "Epoch 130: Train Loss=45.3576, Val Loss=0.4419, Val Acc=72.65%\n",
            "Epoch 131: Train Loss=45.2813, Val Loss=0.4419, Val Acc=72.65%\n",
            "Epoch 132: Train Loss=45.3066, Val Loss=0.4419, Val Acc=72.64%\n",
            "Epoch 133: Train Loss=45.2819, Val Loss=0.4419, Val Acc=72.64%\n",
            "Epoch 134: Train Loss=45.2657, Val Loss=0.4419, Val Acc=72.64%\n",
            "Epoch 135: Train Loss=45.4456, Val Loss=0.4419, Val Acc=72.64%\n",
            "Epoch 136: Train Loss=45.3261, Val Loss=0.4419, Val Acc=72.64%\n",
            "Epoch 137: Train Loss=45.3296, Val Loss=0.4419, Val Acc=72.64%\n",
            "Epoch 138: Train Loss=45.3168, Val Loss=0.4419, Val Acc=72.64%\n",
            "Epoch 139: Train Loss=45.3055, Val Loss=0.4419, Val Acc=72.64%\n",
            "Epoch 140: Train Loss=45.2638, Val Loss=0.4419, Val Acc=72.64%\n",
            "Epoch 141: Train Loss=45.4068, Val Loss=0.4419, Val Acc=72.64%\n",
            "Epoch 142: Train Loss=45.2843, Val Loss=0.4419, Val Acc=72.64%\n",
            "Epoch 143: Train Loss=45.2407, Val Loss=0.4419, Val Acc=72.64%\n",
            "Epoch 144: Train Loss=45.3679, Val Loss=0.4419, Val Acc=72.64%\n",
            "Epoch 145: Train Loss=45.4129, Val Loss=0.4419, Val Acc=72.64%\n",
            "Epoch 146: Train Loss=45.2993, Val Loss=0.4419, Val Acc=72.64%\n",
            "Epoch 147: Train Loss=45.2544, Val Loss=0.4419, Val Acc=72.64%\n",
            "Epoch 148: Train Loss=45.3661, Val Loss=0.4419, Val Acc=72.64%\n",
            "Epoch 149: Train Loss=45.2917, Val Loss=0.4419, Val Acc=72.64%\n",
            "Epoch 150: Train Loss=45.3424, Val Loss=0.4419, Val Acc=72.65%\n",
            "Epoch 151: Train Loss=45.3601, Val Loss=0.4419, Val Acc=72.66%\n",
            "Epoch 152: Train Loss=45.2728, Val Loss=0.4419, Val Acc=72.65%\n",
            "Epoch 153: Train Loss=45.3196, Val Loss=0.4419, Val Acc=72.64%\n",
            "Epoch 154: Train Loss=45.3128, Val Loss=0.4419, Val Acc=72.64%\n",
            "Epoch 155: Train Loss=45.2571, Val Loss=0.4419, Val Acc=72.64%\n",
            "Epoch 156: Train Loss=45.3528, Val Loss=0.4419, Val Acc=72.64%\n",
            "Epoch 157: Train Loss=45.3270, Val Loss=0.4419, Val Acc=72.64%\n",
            "Epoch 158: Train Loss=45.3030, Val Loss=0.4419, Val Acc=72.64%\n",
            "Epoch 159: Train Loss=45.3843, Val Loss=0.4419, Val Acc=72.64%\n",
            "Epoch 160: Train Loss=45.3351, Val Loss=0.4419, Val Acc=72.64%\n",
            "Epoch 161: Train Loss=45.3904, Val Loss=0.4419, Val Acc=72.64%\n",
            "Epoch 162: Train Loss=45.2747, Val Loss=0.4419, Val Acc=72.64%\n",
            "Epoch 163: Train Loss=45.2675, Val Loss=0.4419, Val Acc=72.64%\n",
            "Epoch 164: Train Loss=45.4171, Val Loss=0.4419, Val Acc=72.64%\n",
            "Epoch 165: Train Loss=45.2626, Val Loss=0.4419, Val Acc=72.64%\n",
            "Epoch 166: Train Loss=45.3608, Val Loss=0.4419, Val Acc=72.64%\n",
            "Epoch 167: Train Loss=45.2439, Val Loss=0.4419, Val Acc=72.64%\n",
            "Epoch 168: Train Loss=45.3429, Val Loss=0.4419, Val Acc=72.64%\n",
            "Epoch 169: Train Loss=45.3709, Val Loss=0.4419, Val Acc=72.64%\n",
            "Epoch 170: Train Loss=45.3457, Val Loss=0.4419, Val Acc=72.64%\n",
            "Epoch 171: Train Loss=45.4834, Val Loss=0.4419, Val Acc=72.64%\n",
            "Epoch 172: Train Loss=45.3060, Val Loss=0.4419, Val Acc=72.64%\n",
            "Epoch 173: Train Loss=45.2902, Val Loss=0.4419, Val Acc=72.64%\n",
            "Epoch 174: Train Loss=45.3101, Val Loss=0.4419, Val Acc=72.64%\n",
            "Epoch 175: Train Loss=45.3304, Val Loss=0.4419, Val Acc=72.64%\n",
            "Epoch 176: Train Loss=45.4283, Val Loss=0.4419, Val Acc=72.64%\n",
            "Epoch 177: Train Loss=45.3419, Val Loss=0.4419, Val Acc=72.64%\n",
            "Epoch 178: Train Loss=45.2496, Val Loss=0.4419, Val Acc=72.64%\n",
            "Epoch 179: Train Loss=45.3066, Val Loss=0.4419, Val Acc=72.65%\n",
            "Epoch 180: Train Loss=45.3070, Val Loss=0.4419, Val Acc=72.65%\n",
            "Epoch 181: Train Loss=45.3195, Val Loss=0.4419, Val Acc=72.65%\n",
            "Epoch 182: Train Loss=45.2347, Val Loss=0.4419, Val Acc=72.65%\n",
            "Epoch 183: Train Loss=45.3218, Val Loss=0.4419, Val Acc=72.65%\n",
            "Epoch 184: Train Loss=45.2733, Val Loss=0.4419, Val Acc=72.65%\n",
            "Epoch 185: Train Loss=45.4031, Val Loss=0.4419, Val Acc=72.65%\n",
            "Epoch 186: Train Loss=45.3016, Val Loss=0.4419, Val Acc=72.65%\n",
            "Epoch 187: Train Loss=45.3168, Val Loss=0.4419, Val Acc=72.65%\n",
            "Epoch 188: Train Loss=45.3469, Val Loss=0.4419, Val Acc=72.65%\n",
            "Epoch 189: Train Loss=45.2378, Val Loss=0.4419, Val Acc=72.65%\n",
            "Epoch 190: Train Loss=45.3229, Val Loss=0.4419, Val Acc=72.65%\n",
            "Epoch 191: Train Loss=45.3298, Val Loss=0.4419, Val Acc=72.65%\n",
            "Epoch 192: Train Loss=45.2993, Val Loss=0.4419, Val Acc=72.65%\n",
            "Epoch 193: Train Loss=45.3400, Val Loss=0.4419, Val Acc=72.65%\n",
            "Epoch 194: Train Loss=45.2927, Val Loss=0.4419, Val Acc=72.65%\n",
            "Epoch 195: Train Loss=45.4013, Val Loss=0.4419, Val Acc=72.64%\n",
            "Epoch 196: Train Loss=45.3157, Val Loss=0.4419, Val Acc=72.65%\n",
            "Epoch 197: Train Loss=45.3166, Val Loss=0.4419, Val Acc=72.65%\n",
            "Epoch 198: Train Loss=45.2509, Val Loss=0.4419, Val Acc=72.65%\n",
            "Epoch 199: Train Loss=45.3456, Val Loss=0.4419, Val Acc=72.65%\n",
            "Epoch 200: Train Loss=45.3082, Val Loss=0.4419, Val Acc=72.64%\n"
          ]
        }
      ],
      "source": [
        "train_model(model, train_loader, val_loader, epochs=200)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GOWEGq9r6XiQ",
        "outputId": "6a538531-78f0-40bc-fe56-383ef3d35765"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "FTTransformer(\n",
              "  (embedding): Linear(in_features=8, out_features=128, bias=True)\n",
              "  (transformer): TransformerEncoder(\n",
              "    (layers): ModuleList(\n",
              "      (0-3): 4 x TransformerEncoderLayer(\n",
              "        (self_attn): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
              "        )\n",
              "        (linear1): Linear(in_features=128, out_features=2048, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "        (linear2): Linear(in_features=2048, out_features=128, bias=True)\n",
              "        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "        (dropout1): Dropout(p=0.1, inplace=False)\n",
              "        (dropout2): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (head): Sequential(\n",
              "    (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "    (1): Linear(in_features=128, out_features=2, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 93,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Re-create the model\n",
        "model = FTTransformer(\n",
        "    input_dim=8,  # from embedding.weight shape\n",
        "    dim=128,      # from all transformer and linear layer dimensions\n",
        "    depth=4,      # because there are 2 transformer layers in the checkpoint\n",
        "    heads=8,      # from self-attention weight shape (128 * 3)\n",
        "    dropout=0.1   # or whatever you used before\n",
        ")\n",
        "\n",
        "model.load_state_dict(torch.load(\"best_ft_transformer.pth\"))\n",
        "model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {
        "id": "FTlFrMH0660h"
      },
      "outputs": [],
      "source": [
        "# === Preprocess test set ===\n",
        "member_ids = test_df[\"MemberID\"]  # Save MemberID before dropping\n",
        "\n",
        "drop_cols = ['Unnamed: 0', 'TransactionID', 'MemberID']\n",
        "test_df_clean = test_df.drop(columns=drop_cols)\n",
        "\n",
        "# ✅ Reorder test columns to match train\n",
        "test_df_clean = test_df_clean[train_cols]\n",
        "\n",
        "# Use the same preprocessing steps\n",
        "X_test_imputed = imputer.transform(test_df_clean)\n",
        "\n",
        "X_test_scaled = scaler.transform(X_test_imputed)\n",
        "\n",
        "X_test_tensor = torch.tensor(X_test_scaled, dtype=torch.float32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hQvzr8Ox7Vd7",
        "outputId": "23f88d1f-7026-481c-d33b-faa5ff53bb7a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Predictions saved to ft_transformer_predictions.csv\n"
          ]
        }
      ],
      "source": [
        "# === Predict ===\n",
        "with torch.no_grad():\n",
        "    preds = model(X_test_tensor)\n",
        "    predicted_classes = torch.argmax(preds, dim=1).numpy()\n",
        "\n",
        "# === Export predictions with MemberID ===\n",
        "output = pd.DataFrame({\n",
        "    \"MemberID\": member_ids,\n",
        "    \"next_buy_predicted\": predicted_classes\n",
        "})\n",
        "\n",
        "output.to_csv(\"ft_transformer_predictions.csv\", index=False)\n",
        "print(\"✅ Predictions saved to ft_transformer_predictions.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {
        "id": "BR5-U1OG9bDT"
      },
      "outputs": [],
      "source": [
        "submission_df = pd.read_csv(\"ft_transformer_predictions.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xzMvyVRG9eYy",
        "outputId": "ebca0828-bbd0-4898-83bc-551aa13f330f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 21098 entries, 0 to 21097\n",
            "Data columns (total 2 columns):\n",
            " #   Column    Non-Null Count  Dtype \n",
            "---  ------    --------------  ----- \n",
            " 0   MemberID  21098 non-null  object\n",
            " 1   next_buy  21098 non-null  int64 \n",
            "dtypes: int64(1), object(1)\n",
            "memory usage: 329.8+ KB\n"
          ]
        }
      ],
      "source": [
        "# prompt: rename a column\n",
        "\n",
        "submission_df = submission_df.rename(columns={\"next_buy_predicted\": \"next_buy\"})\n",
        "submission_df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "metadata": {
        "id": "hfu8ETIZ9CQq"
      },
      "outputs": [],
      "source": [
        "submit_df = pd.read_csv(\"https://raw.githubusercontent.com/ZerXXX0/sales-prediction/refs/heads/main/dataset/sample_submission.csv\")\n",
        "# First, create a new dataframe from submission_df with duplicate MemberIDs removed.\n",
        "# We keep the 'last' entry for each member.\n",
        "submission_df_unique = submission_df.drop_duplicates(subset=['MemberID'], keep='last')\n",
        "\n",
        "# Now, create the lookup map from this de-duplicated dataframe.\n",
        "# This will succeed because the 'MemberID' index is now unique.\n",
        "next_buy_lookup = submission_df_unique.set_index('MemberID')['next_buy']\n",
        "\n",
        "# Proceed with the map operation as before. This will now work correctly.\n",
        "submit_df['next_buy'] = submit_df['MemberID'].map(next_buy_lookup)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NeuGBDwD9EnY",
        "outputId": "a835f01d-3747-4169-daf4-ec8b69d0bd68"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 6381 entries, 0 to 6380\n",
            "Data columns (total 2 columns):\n",
            " #   Column    Non-Null Count  Dtype \n",
            "---  ------    --------------  ----- \n",
            " 0   MemberID  6381 non-null   object\n",
            " 1   next_buy  6381 non-null   int64 \n",
            "dtypes: int64(1), object(1)\n",
            "memory usage: 99.8+ KB\n"
          ]
        }
      ],
      "source": [
        "submit_df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {
        "id": "SMxM3dgx9nRB"
      },
      "outputs": [],
      "source": [
        "submit_df.to_csv('submission_FTTransformer.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D0xX8kUj9sqC"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "authorship_tag": "ABX9TyO79SNpICyF8BpddHrcV0I8",
      "gpuType": "T4",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "ultralytics_env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
